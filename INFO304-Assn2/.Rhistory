{
ttdata <- get.imp.train.test(imp, perc.train = perc.train)
train <- ttdata[[1]]
test <- ttdata[[2]]
# Build the model
fit <- rpart(formula = formula, data = train)
#print(fit)
#train.pred <- predict(fit,newdata=train,type="response")
#print(train.pred)
test.pred <- predict(fit,newdata=train,type="response")
#print(test.pred)
#pred.values <- as.factor(test.pred)
#conf.mat <- caret::confusionMatrix(data = pred.vals,
#                                  reference=test$Class)
#(conf.mat$table[1]+conf.mat$table[4])/nrow(test)
}
test.rpart(imps, Class ~ .)
test.rpart <- function(imp, formula,perc.train=0.9)
{
ttdata <- get.imp.train.test(imp, perc.train = perc.train)
train <- ttdata[[1]]
test <- ttdata[[2]]
# Build the model
fit <- rpart(formula = formula, data = train)
#print(fit)
#train.pred <- predict(fit,newdata=train,type="response")
#print(train.pred)
#test.pred <- predict(fit,newdata=train,type="response")
#print(test.pred)
#pred.values <- as.factor(test.pred)
#conf.mat <- caret::confusionMatrix(data = pred.vals,
#                                  reference=test$Class)
#(conf.mat$table[1]+conf.mat$table[4])/nrow(test)
}
test.rpart(imps, Class ~ .)
test.rpart(imps, Class ~ .)
imp.test.real.rf <- function(imp,formula=Class ~ .,perc.train=0.9,...)
{
tt <- get.imp.train.test(imp)
train <- tt[[1]]
test <- tt[[2]]
rf <- randomForest(formula, data=train,...)
pred <- predict(rf,test)
cm <- confusionMatrix(pred,test$Class)
(cm$table[1]+cm$table[4])/nrow(test)
}
test.rpart(imps, Class ~ .)
test.rpart(imps, Class ~ .)
test.rpart <- function(imp, formula,perc.train=0.9)
{
ttdata <- get.imp.train.test(imp, perc.train = perc.train)
train <- ttdata[[1]]
test <- ttdata[[2]]
# Build the model
fit <- rpart(formula = formula, data = train)
#print(fit)
train.pred <- predict(fit,newdata=train,type="response")
print(train.pred)
#test.pred <- predict(fit,newdata=train,type="response")
#print(test.pred)
#pred.values <- as.factor(test.pred)
#conf.mat <- caret::confusionMatrix(data = pred.vals,
#                                  reference=test$Class)
#(conf.mat$table[1]+conf.mat$table[4])/nrow(test)
}
test.rpart(imps, Class ~ .)
imp.test.real.rf <- function(imp,formula=Class ~ .,perc.train=0.9,...)
{
tt <- get.imp.train.test(imp)
train <- tt[[1]]
test <- tt[[2]]
rf <- randomForest(formula, data=train,...)
pred <- predict(rf,test)
cm <- confusionMatrix(pred,test$Class)
(cm$table[1]+cm$table[4])/nrow(test)
}
imp.test.real.rf <- function(imp,formula=Class ~ .,perc.train=0.9,...)
{
tt <- get.imp.train.test(imp)
train <- tt[[1]]
test <- tt[[2]]
rf <- randomForest(formula, data=train,...)
pred <- predict(rf,test)
cm <- confusionMatrix(pred,test$Class)
(cm$table[1]+cm$table[4])/nrow(test)
}
test.rpart <- function(imp, formula,perc.train=0.9)
{
ttdata <- get.imp.train.test(imp, perc.train = perc.train)
train <- ttdata[[1]]
test <- ttdata[[2]]
# Build the model
log.mod <- rpart(formula = formula, data = train)
print(log.mod)
}
test.rpart(imps, Class ~ .)
source("~/Desktop/INFO304/Assn2/logistic.R")
source("~/Desktop/INFO304/Assn2/mice.R")
source("~/Desktop/INFO304/Assn2/utils.R")
source("~/Desktop/INFO304/Assn2/rf.R")
source("~/Desktop/INFO304/Assn2/logistic.R")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = "png")
library(naniar)  # install.packages("naniar") if you don't have it.
library(ggplot2) # package naniar produces ggplots
hepatitis <-read.csv("hepatitis.data")
hepatitis <- data.factorise(hepatitis, factor.cols = c(1,3:14,20))
gg_miss_var(hepatitis,show_pct=TRUE)
print(paste("Number of NA's in Hepatitis:",length(which(is.na(hepatitis)==TRUE))))
vis_miss(hepatitis)
hepatitis <- subset(hepatitis, select = -c(Sex))
library(mice)
imps <- make.imputations(hepatitis) #3.3 Create an imputed data set for the hepatitis data set
x = replicate(100,test.logistic(imps, Class ~ .)) #3.4 Estimate the accuracy of a logistic model using all explanatories
source("~/Desktop/INFO304/Assn2/logistic.R")
##############################################################
# logistic.R
##############################################################
#
# Implements logistic regression using imputed datasets
# and coefficient estimates for variable importance.
##############################################################
source("mice.R") #imputations and training/test splits
setwd("~/Desktop/INFO304/Assn2")
source("~/Desktop/INFO304/Assn2/logistic.R")
library(mice)
imps <- make.imputations(hepatitis) #3.3 Create an imputed data set for the hepatitis data set
x = replicate(100,test.logistic(imps, Class ~ .)) #3.4 Estimate the accuracy of a logistic model using all explanatories
boxplot(x, ylab = "Value", main = "Boxplot Of The Accuracy", col = "white") #boxplot
stripchart(x, method = "jitter", vertical = TRUE, add = TRUE, pch = 19, col = "skyblue")
print(paste("the mean of the accuracy is",mean(x))) #mean accuracy
glm100 = replicate(100,glm.coefficients(imps, Class ~ .))
glm100 = abs(glm100)
sd = apply(glm100, 1, sd)
mean = apply(glm100, 1, mean)
barplot.values = mean/sd
barplot.values <- barplot.values[c(-1)]
barplot.values_ordered <- sort(barplot.values, decreasing = F)
barplot(t(barplot.values), las = 2, xlab = "Variables", ylab = "Standardardized Coefficients")
build.graph.values <- function(dataset, formula){
imps <- make.imputations(dataset)
glm100 <- replicate(100,glm.coefficients(imps, formula))
}
build.graph.values(hepatitis,Class ~.)
build.graph.values <- function(dataset, formula){
imps <- make.imputations(dataset)
glm100 <- replicate(100,glm.coefficients(imps, formula))
glm100 <- abs(glm100)
sd <- apply(glm100,1,sd)
mean <- apply(glm100,1,mean)
barplot.values <- mean/sd
barplot(t(barplot.values), las =2)
}
barplot(t(barplot.values), las =2)
build.graph.values <- function(dataset, formula){
build.graph.values(dataset=hepatitis,formula = Class ~ .)
build.graph.values(hepatitis, Class ~ .)
build.graph.values <- function(dataset, formula){
imps <- make.imputations(dataset)
glm100 <- replicate(100,glm.coefficients(imps, formula))
glm100 <- abs(glm100)
sd <- apply(glm100,1,sd)
mean <- apply(glm100,1,mean)
barplot.values <- mean/sd
barplot.values
}
build.graph.values <- function(dataset, formula){
imps <- make.imputations(dataset)
glm100 <- replicate(100,glm.coefficients(imps, formula))
glm100 <- abs(glm100)
sd <- apply(glm100,1,sd)
mean <- apply(glm100,1,mean)
barplot.values <- mean/sd
print(barplot.values)
}
build.graph.values <- function(dataset, formula){
imps <- make.imputations(dataset)
glm100 <- replicate(100,glm.coefficients(imps, formula))
glm100 <- abs(glm100)
sd <- apply(glm100,1,sd)
mean <- apply(glm100,1,mean)
barplot.values <- mean/sd
print(barplot.values)
}
build.graph.values(hepatitis, Class ~ .)
build.graph.values(hepatitis, Class ~ .)
build.graph.values <- function(dataset, formula){
imps <- make.imputations(dataset)
glm100 <- replicate(100,glm.coefficients(imps, formula))
glm100 <- abs(glm100)
sd <- apply(glm100,1,sd)
mean <- apply(glm100,1,mean)
barplot.values <- mean/sd
barplot(t(barpplot.values))
}
build.graph.values(hepatitis, Class ~ .)
build.graph.values <- function(dataset, formula){
imps <- make.imputations(dataset)
glm100 <- replicate(100,glm.coefficients(imps, formula))
glm100 <- abs(glm100)
sd <- apply(glm100,1,sd)
mean <- apply(glm100,1,mean)
barplot.values <- mean/sd
barplot(t(barplot.values))
}
build.graph.values(hepatitis, Class ~ .)
build.graph.values <- function(dataset, formula){
imps <- make.imputations(dataset)
glm100 <- replicate(100,glm.coefficients(imps, formula))
glm100 <- abs(glm100)
sd <- apply(glm100,1,sd)
mean <- apply(glm100,1,mean)
barplot.values <- mean/sd
barplot.values <- barplot.values[c(-1)]
barplot(t(barplot.values), las =2, xlab = "Variables", ylab = "Standardized Coefficients")
}
build.graph.values(hepatitis, Class ~ .)
graph1 <- build.graph.values(hepatitis, Class ~ .)
build.graph.values(hepatitis, Class ~ .)
graph1 <- build.graph.values(hepatitis, Class ~ .)
build.graph.values(hepatitis, Class ~ .)
build.graph.values(hepatitis, Class ~ .)
build.graph.values(hepatitis, Class ~ .)
build.graph.values(hepatitis, Class ~ .)
replicate(5,build.graph.values(hepatitis, Class ~ .))
variable.importance <- function(imp,formula,var.col=2,perc.train=0.9)
{
# get the training/test data randomly from one imputed datasets
ttdata <- get.imp.train.test(imp,perc.train=perc.train)
train <- ttdata[[1]]
test <- ttdata[[2]]
# Build the model...
log.mod <- glm(formula, data=train, family="binomial")
#
# and now find the threshold to maximise accuracy...
# uses the same training data (since we can't use test)
train.pred <- predict(log.mod,newdata=train,type="response")
# But now we can get back the threshold
threshold <- fast.threshold(train$Class,train.pred)
# and now test the model...
test.pred <- predict(log.mod,newdata=test,type="response")
# and use the threshold to calculate the final predicted values
#
pred.vals <- as.factor(ifelse(test.pred >=threshold,2,1))
conf.mat <- caret::confusionMatrix(data = pred.vals,
reference=test$Class)
# computes the logistic model accuracy between 0 and 1
original.test <- (conf.mat$table[1]+conf.mat$table[4])/nrow(test)
#print(original.test)
#imputing the data
newr <- sample(1:nrow(test),nrow(test), replace = FALSE)
test[,var.col] <- test[newr,var.col] # imputed dataset
log.mod <- glm(formula, data=train, family="binomial")
#
# and now find the threshold to maximise accuracy...
# uses the same training data (since we can't use test)
train.pred <- predict(log.mod,newdata=train,type="response")
# But now we can get back the threshold
threshold <- fast.threshold(train$Class,train.pred)
# and now test the model...
test.pred <- predict(log.mod,newdata=test,type="response")
# and use the threshold to calculate the final predicted values
#
pred.vals <- as.factor(ifelse(test.pred >=threshold,2,1))
conf.mat <- caret::confusionMatrix(data = pred.vals,
reference=test$Class)
# Return accuracy between 0 and 1
imputed.test <- (conf.mat$table[1]+conf.mat$table[4])/nrow(test)
#print(imputed.test)
#calculate percentage increase
dif <- imputed.test-original.test
percentage.change <- dif/imputed.test*100
percentage.change
}
a = collect.var.imp(imps, Class ~ .)
colMeans(a)
log(a)
colMeans(a)
a = a^2
colMeans(a)
a = collect.var.imp(imps, Class ~ .)
variable.importance <- function(imp,formula,var.col=2,perc.train=0.9)
{
# get the training/test data randomly from one imputed datasets
ttdata <- get.imp.train.test(imp,perc.train=perc.train)
train <- ttdata[[1]]
test <- ttdata[[2]]
# Build the model...
log.mod <- glm(formula, data=train, family="binomial")
#
# and now find the threshold to maximise accuracy...
# uses the same training data (since we can't use test)
train.pred <- predict(log.mod,newdata=train,type="response")
# But now we can get back the threshold
threshold <- fast.threshold(train$Class,train.pred)
# and now test the model...
test.pred <- predict(log.mod,newdata=test,type="response")
# and use the threshold to calculate the final predicted values
#
pred.vals <- as.factor(ifelse(test.pred >=threshold,2,1))
conf.mat <- caret::confusionMatrix(data = pred.vals,
reference=test$Class)
# computes the logistic model accuracy between 0 and 1
original.test <- (conf.mat$table[1]+conf.mat$table[4])/nrow(test)
#print(original.test)
#imputing the data
newr <- sample(1:nrow(test),nrow(test), replace = FALSE)
test[,var.col] <- test[newr,var.col] # imputed dataset
log.mod <- glm(formula, data=train, family="binomial")
#
# and now find the threshold to maximise accuracy...
# uses the same training data (since we can't use test)
train.pred <- predict(log.mod,newdata=train,type="response")
# But now we can get back the threshold
threshold <- fast.threshold(train$Class,train.pred)
# and now test the model...
test.pred <- predict(log.mod,newdata=test,type="response")
# and use the threshold to calculate the final predicted values
#
pred.vals <- as.factor(ifelse(test.pred >=threshold,2,1))
conf.mat <- caret::confusionMatrix(data = pred.vals,
reference=test$Class)
# Return accuracy between 0 and 1
imputed.test <- (conf.mat$table[1]+conf.mat$table[4])/nrow(test)
#print(imputed.test)
#calculate percentage increase
dif <- imputed.test-original.test
percentage.change <- dif/imputed.test*100
imputed.tes
}
a = collect.var.imp(imps, Class ~ .)
variable.importance <- function(imp,formula,var.col=2,perc.train=0.9)
{
# get the training/test data randomly from one imputed datasets
ttdata <- get.imp.train.test(imp,perc.train=perc.train)
train <- ttdata[[1]]
test <- ttdata[[2]]
# Build the model...
log.mod <- glm(formula, data=train, family="binomial")
#
# and now find the threshold to maximise accuracy...
# uses the same training data (since we can't use test)
train.pred <- predict(log.mod,newdata=train,type="response")
# But now we can get back the threshold
threshold <- fast.threshold(train$Class,train.pred)
# and now test the model...
test.pred <- predict(log.mod,newdata=test,type="response")
# and use the threshold to calculate the final predicted values
#
pred.vals <- as.factor(ifelse(test.pred >=threshold,2,1))
conf.mat <- caret::confusionMatrix(data = pred.vals,
reference=test$Class)
# computes the logistic model accuracy between 0 and 1
original.test <- (conf.mat$table[1]+conf.mat$table[4])/nrow(test)
#print(original.test)
#imputing the data
newr <- sample(1:nrow(test),nrow(test), replace = FALSE)
test[,var.col] <- test[newr,var.col] # imputed dataset
log.mod <- glm(formula, data=train, family="binomial")
#
# and now find the threshold to maximise accuracy...
# uses the same training data (since we can't use test)
train.pred <- predict(log.mod,newdata=train,type="response")
# But now we can get back the threshold
threshold <- fast.threshold(train$Class,train.pred)
# and now test the model...
test.pred <- predict(log.mod,newdata=test,type="response")
# and use the threshold to calculate the final predicted values
#
pred.vals <- as.factor(ifelse(test.pred >=threshold,2,1))
conf.mat <- caret::confusionMatrix(data = pred.vals,
reference=test$Class)
# Return accuracy between 0 and 1
imputed.test <- (conf.mat$table[1]+conf.mat$table[4])/nrow(test)
#print(imputed.test)
#calculate percentage increase
dif <- imputed.test-original.test
percentage.change <- dif/imputed.test*100
imputed.test
}
a = collect.var.imp(imps, Class ~ .)
colMeans(a)
a = collect.var.imp(imps, Class ~ .)
a.means <- colMeans(a)
barplot(t(a.means), las =2, xlab = "Variables", ylab = "Standardized Coefficients")
variable.importance <- function(imp,formula,var.col=2,perc.train=0.9)
{
# get the training/test data randomly from one imputed datasets
ttdata <- get.imp.train.test(imp,perc.train=perc.train)
train <- ttdata[[1]]
test <- ttdata[[2]]
# Build the model...
log.mod <- glm(formula, data=train, family="binomial")
#
# and now find the threshold to maximise accuracy...
# uses the same training data (since we can't use test)
train.pred <- predict(log.mod,newdata=train,type="response")
# But now we can get back the threshold
threshold <- fast.threshold(train$Class,train.pred)
# and now test the model...
test.pred <- predict(log.mod,newdata=test,type="response")
# and use the threshold to calculate the final predicted values
#
pred.vals <- as.factor(ifelse(test.pred >=threshold,2,1))
conf.mat <- caret::confusionMatrix(data = pred.vals,
reference=test$Class)
# computes the logistic model accuracy between 0 and 1
original.test <- (conf.mat$table[1]+conf.mat$table[4])/nrow(test)
#print(original.test)
#imputing the data
newr <- sample(1:nrow(test),nrow(test), replace = FALSE)
test[,var.col] <- test[newr,var.col] # imputed dataset
log.mod <- glm(formula, data=train, family="binomial")
#
# and now find the threshold to maximise accuracy...
# uses the same training data (since we can't use test)
train.pred <- predict(log.mod,newdata=train,type="response")
# But now we can get back the threshold
threshold <- fast.threshold(train$Class,train.pred)
# and now test the model...
test.pred <- predict(log.mod,newdata=test,type="response")
# and use the threshold to calculate the final predicted values
#
pred.vals <- as.factor(ifelse(test.pred >=threshold,2,1))
conf.mat <- caret::confusionMatrix(data = pred.vals,
reference=test$Class)
# Return accuracy between 0 and 1
imputed.test <- (conf.mat$table[1]+conf.mat$table[4])/nrow(test)
#print(imputed.test)
#calculate percentage increase
dif <- imputed.test-original.test
percentage.change <- dif/imputed.test*100
percentage.change
}
a = collect.var.imp(imps, Class ~ .)
a.means <- colMeans(a)
barplot(t(a.means), las =2, xlab = "Variables", ylab = "Standardized Coefficients")
a = collect.var.imp(imps, Class ~ .)
a.means <- colMeans(a)
barplot(t(a.means), las =2, xlab = "Variables", ylab = "Standardized Coefficients")
a = collect.var.imp(imps, Class ~ .)
a.means <- colMeans(a)
barplot(t(a.means), las =2, xlab = "Variables", ylab = "Standardized Coefficients")
a = collect.var.imp(imps, Class ~ .)
a.means <- colMeans(a)
barplot(t(a.means), las =2, xlab = "Variables", ylab = "Standardized Coefficients")
a = collect.var.imp(imps, Class ~ .)
a.means <- colMeans(a)
barplot(t(a.means), las =2, xlab = "Variables", ylab = "Standardized Coefficients")
a = collect.var.imp(imps, Class ~ .)
a.means <- colMeans(a)
barplot(t(a.means), las =2, xlab = "Variables", ylab = "Standardized Coefficients")
a = collect.var.imp(imps, Class ~ .)
a.means <- colMeans(a)
barplot(t(a.means), las =2, xlab = "Variables", ylab = "Standardized Coefficients")
a = collect.var.imp(imps, Class ~ .)
a.means <- colMeans(a)
barplot(t(a.means), las =2, xlab = "Variables", ylab = "Standardized Coefficients")
a = collect.var.imp(imps, Class ~ .)
a.means <- colMeans(a)
barplot(t(a.means), las =2, xlab = "Variables", ylab = "Standardized Coefficients")
build.permuted.graph <- function(imp, formula){
var.imps <- collect.var.imp(imp, formula)
col.means <- colMeans(var.imps)
barplot(t(col.means),xlab = "Variables", ylab = "Percentage Change")
}
replicate(5,build.permuted.graph(imps,Class ~ .))
build.permuted.graph <- function(imp, formula){
var.imps <- collect.var.imp(imp, formula)
col.means <- colMeans(var.imps)
barplot(t(col.means),xlab = "Variables", ylab = "Percentage Change")
}
replicate(5,build.permuted.graph(imps,Class ~ .))
# explanatory variables.
# IN: imp - the imputed dataset
#     cols - defaults to all columns
# perc.train - percentage of training data
# num.trials - Number of times to repeat variable importance test
# OP: Calls variable.importance *num.trials* times and collects up
#  the resulting accuracy for the columns specified in *cols*
# OUT: A table of results - number of columns = number of columns specified
#      by cols, number of rows = num.trials.
###################################################################
collect.var.imp <- function(imp,formula,cols=2:ncol(imp[[1]]),
perc.train=0.9, num.trials=100)
{
res <- NULL
for (i in 1:length(cols))
{
# Do lazy method of binding up results....
res <- cbind(res,
replicate(num.trials,variable.importance(imp,
formula,var.col=cols[i])))
}
res
}
coef(log.mod)
build.permuted.graph <- function(imp, formula){
var.imps <- collect.var.imp(imp, formula)
col.means <- colMeans(var.imps)
barplot(t(col.means),xlab = "Variables", ylab = "Percentage Change")
}
replicate(5,build.permuted.graph(imps,Class ~ .))
col.means
var.imps <- collect.var.imp(imp, formula)
col.means <- colMeans(var.imps)
colMeans(collect.var.imp(imp, formula))
colMeans(collect.var.imp(imps, Class ~ . ))
source("~/Desktop/INFO304/Assn2/logistic.R")
source("~/Desktop/INFO304/Assn2/utils.R")
source("~/Desktop/INFO304/Assn2/mice.R")
source("~/Desktop/INFO304/Assn2/rf.R")
